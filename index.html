<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Tahmid Hasan</title>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
	    <meta property="og:title" content="Tahmid Hasan" />
	    <meta property="og:image" content="https://tahmid04.github.io/img/tahmid.jpg" />
	    <meta http-equiv="X-UA-Compatible" content="IE=edge">
	    <meta name="author" content="Tahmid Hasan">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="shortcut icon" type="image/png" href="favicon.ico"/>
        <link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.0.0-beta/css/bootstrap.min.css'>
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" href="css/style.css">
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    
        <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js'></script>
        <script src='https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.0.8/popper.min.js'></script>
        <script src='https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.0.0-beta/js/bootstrap.min.js'></script>
        <script  src="js/script.js"></script>
    </head>
    <body>
    	<style>
		pre {
    		text-align: left;
    		white-space: pre-line;
  		}
		</style>

        <div class="container mt-5" >
            <nav class="navbar navbar-expand-lg navbar-dark fixed-top bg-dark">
                <div class="container">
                        <button id="myButton" class="navbar-toggler" type="button" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
                                <span class="navbar-toggler-icon"></span>
                        </button>
                        <div class="collapse navbar-collapse" id="navbarCollapse">
                        <ul class="navbar-nav">
                        <li class="nav-item active">
                            <a class="nav-link" href="#about">About <span class="sr-only">(current)</span></a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#publications">Publications</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#ongoing-projects">Ongoing Projects</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#education">Education</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#professional-experience">Professional Experience</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#honors-awards">Honors & Awards</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#teaching-experience">Teaching</a>
                        </li>
                        </ul>
                        </div>
                </div>
            </nav>
    
            <div class="row mb-3" id="about">
                <div class="col"><br/>
                    <h1>Tahmid Hasan</h1>
                </div>
            </div>

            <div class="row" style="font-size:110%;">
                <div class="col-md-4 order-md-2">
                    <img src="img/tahmid.jpg" alt="Tahmid" class="img-fluid rounded">
                </div>
                <div class="col-md-8 order-md-1" >
                    <p>
                        Hi! I am a second year Masters student and also a lecturer at the <a href="https://cse.buet.ac.bd/" target="_blank">Department of Computer Science and Engineering</a> from <a href="https://www.buet.ac.bd/" target="_blank">Bangladesh University of Engineering and Technology (BUET)</a>. I am fortunate to be supervised by Prof. <a href="http://rifatshahriyar.github.io/" target="_blank">Rifat Shahriyar</a> and am affiliated with the BUET CSE NLP group. 

                    </p>
                    <p>
                        Previously, I got my Bachelors degree from the same department and worked on Bioinformatics and Networking supervised by Prof. <a href="https://msrahman.buet.ac.bd/" target="_blank">M. Sohel Rahman</a>.
                    </p>
                    <p>
                        My current research interests are in low-resource, multilingual, and cross-lingual natural language processing. I am particularly interested in efficient utilization of compute and data in scenarios where one or both are limited. My long term goal is to create reliable NLP systems that can learn to communicate with speakers of any language through interaction.
                    </p>
                    <p>
                        Over the last two years, I have worked on multiple projects on machine translation, natural language understanding, text summarization, and NLP for programming languages.
                    </p>
                </div>
            </div>

            <div class="row" style="font-size:110%;">
                <div class="col">
                    <p>
                        Email: tahmidhasan [<a href="https://en.wikipedia.org/wiki/At_sign" target="_blank">at</a>] cse.buet.ac.bd
                    </p>
                    <p>
                        Links:
                        [<a href="pdf/CV_Tahmid_Hasan.pdf" target="_blank">Curriculum Vitae</a>] [<a href="https://scholar.google.com/citations?user=UfXXUpQAAAAJ&hl=en" target="_blank">Google Scholar</a>] [<a href="https://twitter.com/tahmidhasan04" target="_blank">Twitter</a>] [<a href="https://github.com/Tahmid04" target="_blank">Github</a>]
                    </p>
                </div>
            </div>
            
            <hr>

            <div class="row" id="publications">
                <div class="col">
                    <h2>Publications</h2>
                    <ol>
                        <li>

                            <a href="https://aclanthology.org/2021.findings-acl.413.pdf" target="_blank">
                                <b>XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages</b>
                            </a>
                            <br/>
                            <b>Tahmid Hasan</b>,  Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, Rifat Shahriyar
                            <br/>
                            In <a href="https://aclanthology.org/volumes/2021.findings-acl/" target="_blank">
                                <b>
                                    Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 </b></a>
                            <br/>               

                            [<a href="#" onclick="$('#acl2021_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#acl2021_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://github.com/csebuetnlp/xl-sum" target="_blank">code</a>]
                            <div id="acl2021_abstract" class="abstract" style="display:none;">
                                <p>
                                    Contemporary works on abstractive text summarization have focused primarily on high-resource languages like English, mostly due to the limited availability of datasets for low/mid-resource ones. In this work, we present XL-Sum, a comprehensive and diverse dataset comprising 1 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 44 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation. We fine-tune mT5, a state-of-the-art pretrained multilingual model, with XL-Sum and experiment on multilingual and low-resource summarization tasks. XL-Sum induces competitive results compared to the ones obtained using similar monolingual datasets: we show higher than 11 ROUGE-2 scores on 10 languages we benchmark on, with some of them exceeding 15, as obtained by multilingual training. Additionally, training on low-resource languages individually also provides competitive performance. To the best of our knowledge, XL-Sum is the largest abstractive summarization dataset in terms of the number of samples collected from a single source and the number of languages covered. We are releasing our dataset and models to encourage future research on multilingual abstractive summarization. The resources can be found at <a href="https://github.com/csebuetnlp/xl-sum" target="_blank">https://github.com/csebuetnlp/xl-sum</a>.
                                </p>
                            </div>
                            <div id="acl2021_bib" class="bib" style="display:none;">
                                <pre>
                                    @inproceedings{hasan-etal-2021-xl,
                                        title = "{XL}-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages",
                                        author = "Hasan, Tahmid  and
                                            Bhattacharjee, Abhik  and
                                            Islam, Md. Saiful  and
                                            Mubasshir, Kazi  and
                                            Li, Yuan-Fang  and
                                            Kang, Yong-Bin  and
                                            Rahman, M. Sohel  and
                                            Shahriyar, Rifat",
                                        booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                                        month = aug,
                                        year = "2021",
                                        address = "Online",
                                        publisher = "Association for Computational Linguistics",
                                        url = "https://aclanthology.org/2021.findings-acl.413",
                                        doi = "10.18653/v1/2021.findings-acl.413",
                                        pages = "4693--4703",
                                    }
                                </pre>
                            </div>
                        </li>
                        <br/>


                        <li>

                            <a href="https://aclanthology.org/2021.findings-acl.18.pdf" target="_blank">
                                <b>CoDesc: A Large Codeâ€“Description Parallel Dataset</b>
                            </a>
                            <br/>
                            Masum Hasan, Tanveer Muttaqueen, Abdullah Al Ishtiaq, Kazi Sajeed Mehrab, Md. Mahim Anjum Haque, <b>Tahmid Hasan</b>, Wasi Ahmad, Anindya Iqbal, Rifat Shahriyar
                            <br/>
                            In <a href="https://aclanthology.org/volumes/2021.findings-acl/" target="_blank">
                                <b>
                                    Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 </b></a>
                            <br/>

                            [<a href="#" onclick="$('#aclcodesc2021_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#aclcodesc2021_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://github.com/csebuetnlp/CoDesc" target="_blank">code</a>]
                            <div id="aclcodesc2021_abstract" class="abstract" style="display:none;">
                                <p>
                                    Translation between natural language and source code can help software development by enabling developers to comprehend, ideate, search, and write computer programs in natural language. Despite growing interest from the industry and the research community, this task is often difficult due to the lack of large standard datasets suitable for training deep neural models, standard noise removal methods, and evaluation benchmarks. This leaves researchers to collect new small-scale datasets, resulting in inconsistencies across published works. In this study, we present CoDesc -- a large parallel dataset composed of 4.2 million Java methods and natural language descriptions. With extensive analysis, we identify and remove prevailing noise patterns from the dataset. We demonstrate the proficiency of CoDesc in two complementary tasks for code-description pairs: code summarization and code search. We show that the dataset helps improve code search by up to 22\% and achieves the new state-of-the-art in code summarization. Furthermore, we show CoDesc's effectiveness in pre-training--fine-tuning setup, opening possibilities in building pretrained language models for Java. To facilitate future research, we release the dataset, a data processing tool, and a benchmark at <a href="https://github.com/csebuetnlp/CoDesc" target="_blank">https://github.com/csebuetnlp/CoDesc</a>.
                                </p>
                            </div>
                            <div id="aclcodesc2021_bib" class="bib" style="display:none;">
                                <pre>
                                    @inproceedings{hasan-etal-2021-codesc,
                                        title = "{C}o{D}esc: A Large Code{--}Description Parallel Dataset",
                                        author = "Hasan, Masum  and
                                          Muttaqueen, Tanveer  and
                                          Ishtiaq, Abdullah Al  and
                                          Mehrab, Kazi Sajeed  and
                                          Haque, Md. Mahim Anjum  and
                                          Hasan, Tahmid  and
                                          Ahmad, Wasi  and
                                          Iqbal, Anindya  and
                                          Shahriyar, Rifat",
                                        booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                                        month = aug,
                                        year = "2021",
                                        address = "Online",
                                        publisher = "Association for Computational Linguistics",
                                        url = "https://aclanthology.org/2021.findings-acl.18",
                                        doi = "10.18653/v1/2021.findings-acl.18",
                                        pages = "210--218",
                                    }
                                </pre>
                            </div>
                        </li>
                        <br/>

                        <li>

                            <a href="https://aclanthology.org/2020.emnlp-main.207.pdf" target="_blank">
                                <b>Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New Datasets for Bengali-English Machine Translation</b>
                            </a>
                            <br/>
                            <b>Tahmid Hasan</b>,  Abhik Bhattacharjee,  Kazi Samin, Masum Hasan, Madhusudan Basak, M. Sohel Rahman, Rifat Shahriyar     
                            <br/>
                            In <a href="https://aclanthology.org/volumes/2020.emnlp-main/" target="_blank">
                                <b>
                                    Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) </b></a>
                            <br/>

                            [<a href="#" onclick="$('#emnlp2020_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#emnlp2020_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://github.com/csebuetnlp/banglanmt" target="_blank">code</a>]
                            <div id="emnlp2020_abstract" class="abstract" style="display:none;">
                                <p>
                                    Despite being the seventh most widely spoken language in the world, Bengali has received much less attention in machine translation literature due to being low in resources. Most publicly available parallel corpora for Bengali are not large enough; and have rather poor quality, mostly because of incorrect sentence alignments resulting from erroneous sentence segmentation, and also because of a high volume of noise present in them. In this work, we build a customized sentence segmenter for Bengali and propose two novel methods for parallel corpus creation on low-resource setups: aligner ensembling and batch filtering. With the segmenter and the two methods combined, we compile a high-quality Bengali-English parallel corpus comprising of 2.75 million sentence pairs, more than 2 million of which were not available before. Training on neural models, we achieve an improvement of more than 9 BLEU score over previous approaches to Bengali-English machine translation. We also evaluate on a new test set of 1000 pairs made with extensive quality control. We release the segmenter, parallel corpus, and the evaluation set, thus elevating Bengali from its low-resource status. To the best of our knowledge, this is the first ever large scale study on Bengali-English machine translation. We believe our study will pave the way for future research on Bengali-English machine translation as well as other low-resource languages. Our data and code are available at <a href="https://github.com/csebuetnlp/banglanmt" target="_blank">https://github.com/csebuetnlp/banglanmt</a>.
                                </p>
                            </div>
                            <div id="emnlp2020_bib" class="bib" style="display:none;">
                                <pre>
                                    @inproceedings{hasan-etal-2020-low,
                                        title = "Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New Datasets for {B}engali-{E}nglish Machine Translation",
                                        author = "Hasan, Tahmid  and
                                          Bhattacharjee, Abhik  and
                                          Samin, Kazi  and
                                          Hasan, Masum  and
                                          Basak, Madhusudan  and
                                          Rahman, M. Sohel  and
                                          Shahriyar, Rifat",
                                        booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                                        month = nov,
                                        year = "2020",
                                        address = "Online",
                                        publisher = "Association for Computational Linguistics",
                                        url = "https://aclanthology.org/2020.emnlp-main.207",
                                        doi = "10.18653/v1/2020.emnlp-main.207",
                                        pages = "2612--2623",                                        
                                    }
                                </pre>
                            </div>
                        </li>
                        <br/>


                        <li>

                            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8214213" target="_blank">
                                <b>Using Adaptive Heartbeat Rate on Long-Lived TCP Connections</b>
                            </a>
                            <br/>
                            M. Saifur Rahman, Md. Yusuf Sarwar Uddin, <b>Tahmid Hasan</b>, M. Sohel Rahman, M. Kaykobad
                            <br/>
                            In <a href="https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=8291715" target="_blank">
                                <b>
                                    IEEE/ACM Transactions on Networking (Volume: 26, Issue: 1, Feb. 2018) </b></a>
                            <br/>

                            [<a href="#" onclick="$('#ton2018_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#ton2018_abstract').toggle();return false;">abstract</a>]
                            <div id="ton2018_abstract" class="abstract" style="display:none;">
                                <p>
                                    In this paper, we propose techniques for dynamically adjusting heartbeat or keep-alive interval of long-lived TCP connections, particularly the ones that are used in push notification service in mobile platforms. When a device connects to a server using TCP, often times the connection is established through some sort of middle-box, such as NAT, proxy, firewall, and so on. When such a connection is idle for a long time, it may get torn down due to binding timeout of the middle-box. To keep the connection alive, the client device needs to send keep-alive packets through the connection when it is otherwise idle. To reduce resource consumption, the keep-alive packet should preferably be sent at the farthest possible time within the binding timeout. Due to varied settings of different network equipments, the binding timeout will not be identical in different networks. Hence, the heartbeat rate used in different networks should be changed dynamically. We propose a set of iterative probing techniques, namely binary, exponential, and composite search, that detect the middle-box binding timeout with varying degree of accuracy; and in the process, keeps improving the keep-alive interval used by the client device. We also analytically derive performance bounds of these techniques. To the best of our knowledge, ours is the first work that systematically studies several techniques to dynamically improve keep-alive interval. To this end, we run experiments in simulation as well as make a real implementation on android to demonstrate the proof-of-concept of the proposed schemes.
                                </p>
                            </div>
                            <div id="ton2018_bib" class="bib" style="display:none;">
                                <pre>
                                    @article{rahman2017using,
                                        title={Using adaptive heartbeat rate on long-lived TCP connections},
                                        author={Rahman, M Saifur and Uddin, Md Yusuf Sarwar and Hasan, Tahmid and Rahman, M Sohel and Kaykobad, M},
                                        journal={IEEE/ACM Transactions on Networking},
                                        volume={26},
                                        number={1},
                                        pages={203--216},
                                        year={2017},
                                        publisher={IEEE}
                                      }
                                </pre>
                            </div>
                        </li>
                        <br/>

                    </ol>
       
                    <h5>Pre-prints:</h5>
                    <ol>
                        <li>

                            <a href="https://arxiv.org/pdf/2101.00204.pdf" target="_blank">
                                <b>BanglaBERT: Combating Embedding Barrier in Multilingual Models for Low-Resource Language Understanding</b>
                            </a>
                            <br/>
                            Abhik Bhattacharjee*, <b>Tahmid Hasan</b>*, Kazi Samin, Md Saiful Islam, M. Sohel Rahman, Anindya Iqbal, Rifat Shahriyar
                            <br/>
                            *: Equal contribution
                            <br/>
                            <a href="https://arxiv.org/abs/2101.00204" target="_blank">
                                <b>
                                    ArXiv Pre-print, 2021 </b></a>
                            <br/>

                            [<a href="#" onclick="$('#banglabert_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#banglabert_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://github.com/csebuetnlp/banglabert" target="_blank">code</a>]
                            <div id="banglabert_abstract" class="abstract" style="display:none;">
                                <p>
                                    In this paper, we introduce ``Embedding Barrier'', a phenomenon that limits the monolingual performance of multilingual models on low-resource languages having unique typologies. We build `BanglaBERT', a Bangla language model pretrained on 18.6 GB Internet-crawled data and benchmark on five standard NLU tasks. We discover a significant drop in the performance of the state-of-the-art multilingual model (XLM-R) from BanglaBERT and attribute this to the Embedding Barrier through comprehensive experiments. We identify that a multilingual model's performance on a low-resource language is hurt when its writing script is not similar to any of the high-resource languages. To tackle the barrier, we propose a straightforward solution by transcribing languages to a common script, which can effectively improve the performance of a multilingual model for the Bangla language. As a bi-product of the standard NLU benchmarks, we introduce a new downstream dataset on natural language inference (NLI) and show that BanglaBERT outperforms previous state-of-the-art results on all tasks by up to 3.5\%. We are making the BanglaBERT language model and the new Bangla NLI dataset publicly available in the hope of advancing the community. The resources can be found at <a href="https://github.com/csebuetnlp/banglabert" target="_blank">https://github.com/csebuetnlp/banglabert</a>.
                                </p>
                            </div>
                            <div id="banglabert_bib" class="bib" style="display:none;">
                                <pre>
                                    @article{bhattacharjee2021banglabert,
                                        title={BanglaBERT: Combating Embedding Barrier in Multilingual Models for Low-Resource Language Understanding},
                                        author={Bhattacharjee, Abhik and Hasan, Tahmid and Samin, Kazi and Rahman, M Sohel and Iqbal, Anindya and Shahriyar, Rifat},
                                        journal={arXiv preprint arXiv:2101.00204},
                                        year={2021}
                                    }
                                </pre>
                            </div>
                        </li>
                        <br/>

                        <li>

                            <a href="https://arxiv.org/pdf/2104.08017.pdf" target="_blank">
                                <b>BERT2Code: Can Pretrained Language Models be Leveraged for Code Search?</b>
                            </a>
                            <br/>
                            Abdullah Al Ishtiaq, Masum Hasan, Md. Mahim Anjum Haque, Kazi Sajeed Mehrab, Tanveer Muttaqueen, <b>Tahmid Hasan</b>, Anindya Iqbal, Rifat Shahriyar
                            <br/>
                            <a href="https://arxiv.org/abs/2104.08017" target="_blank">
                                <b>
                                    ArXiv Pre-print, 2021 </b></a>
                            <br/>               

                            [<a href="#" onclick="$('#bert2code_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#bert2code_abstract').toggle();return false;">abstract</a>]
                            <div id="bert2code_abstract" class="abstract" style="display:none;">
                                <p>
                                    Millions of repetitive code snippets are submitted to code repositories every day. To search from these large codebases using simple natural language queries would allow programmers to ideate, prototype, and develop easier and faster. Although the existing methods have shown good performance in searching codes when the natural language description contains keywords from the code, they are still far behind in searching codes based on the semantic meaning of the natural language query and semantic structure of the code. In recent years, both natural language and programming language research communities have created techniques to embed them in vector spaces. In this work, we leverage the efficacy of these embedding models using a simple, lightweight 2-layer neural network in the task of semantic code search. We show that our model learns the inherent relationship between the embedding spaces and further probes into the scope of improvement by empirically analyzing the embedding methods. In this analysis, we show that the quality of the code embedding model is the bottleneck for our model's performance, and discuss future directions of study in this area.
                                </p>
                            </div>
                            <div id="bert2code_bib" class="bib" style="display:none;">
                                <pre>
                                    @article{ishtiaq2021bert2code,
                                        title={BERT2Code: Can Pretrained Language Models be Leveraged for Code Search?},
                                        author={Ishtiaq, Abdullah Al and Hasan, Masum and Haque, Md and Anjum, Mahim and Mehrab, Kazi Sajeed and Muttaqueen, Tanveer and Hasan, Tahmid and Iqbal, Anindya and Shahriyar, Rifat},
                                        journal={arXiv preprint arXiv:2104.08017},
                                        year={2021}
                                    }
                                </pre>
                            </div>
                        </li>
                        <br/>

                    </ol>
                </div>
            </div>

            <hr>

            <div class="row" id="ongoing-projects">
                <div class="col">
                    <h2>Ongoing Projects</h2>
                    <ol>
                        <li>
                            <b>Adapting XL-Sum for Cross-Lingual Summarization</b>
                            [<a href="#" onclick="$('#xlcross_statement').toggle();return false;">statement</a>]                            
                            <div id="xlcross_statement" class="statement" style="display:none;">
                                <p>
                                    The target language of a multilingual model for cross-lingual summarization is limited to only the language it is fine-tuned on, and we have observed that fine-tuning with multiple languages without cross-lingual supervision can not help control the language of the generated summaries. In this work, we are aiming to generate summaries in any target language for a given article by fine-tuning multilingual models with explicit (albeit limited) cross-lingual signals. By aligning identical articles across languages via cross-lingual retrieval on the <a href="https://github.com/csebuetnlp/xl-sum" target="_blank">XL-Sum dataset</a>, coupled with a multi-stage sampling technique, we are aiming to perform large-scale cross-lingual summarization for 45 languages.
                                </p>
                            </div>
                            <br/>
                            Supervisors: Dr. <a href="https://research.monash.edu/en/persons/yuan-fang-li" target="_blank">Yuan-Fang Li</a> and Prof. Rifat Shahriyar<br/>
                            <br/>
                        </li>

                        <li>
                            <b>Paraphrase Generation via Knowledge Distillation from Machine Translation Models</b>
                            [<a href="#" onclick="$('#kd_statement').toggle();return false;">statement</a>]                            
                            <div id="kd_statement" class="statement" style="display:none;">
                                <p>
                                    Synthetic paraphrase datasets are typically generated with round-trip machine translation. Since these back-translation-based data generation approaches have been shown to generate appropriate paraphrases, in this work, we are trying to directly distill the knowledge of translation models into a paraphrase generation model. We are aiming to use two teachers, namely a forward translation model and a backward translation model, to distill two types of knowledge into the paraphrase model: the cross-attention distribution and the output distribution. In constrast to traditional knowledge distillation, here we have two teacher models instead of one and the task of the student model is different from the teacher models.
                                </p>
                            </div>
                            <br/>
                            Supervisors: Dr. <a href="https://wasiahmad.github.io/" target="_blank">Wasi Uddin  Ahmad</a> and Prof. Rifat Shahriyar<br/>
                            <br/>
                        </li>
                        
                    </ol>
                </div>
            </div>

            <hr>

            <div class="row" id="education">
                <div class="col">
                    <h2>Education</h2>
                    <ul>
                        <li>
                            <b>(July 2019 - Present) Master of Science in Computer Science and Engineering (Exp. June 2022)</b><br/>
                            Department of Computer Science and Engineering<br/>
                            Bangladesh University of Engineering and Technology (BUET)<br/>
                            Supervisor: Prof. Rifat Shahriyar<br/>
                            CGPA: 3.90/4.00<br/>
                            <br/>
                        </li>

                        <li>
                            <b>(February 2015 - April 2019) Bachelor of Science in Computer Science and Engineering</b><br/>
                            Department of Computer Science and Engineering<br/>
                            Bangladesh University of Engineering and Technology (BUET)<br/>
                            CGPA: 3.98/4.00<br/>
                            Thesis Supervisor: Prof. M. Sohel Rahman
                            <br/>
                        </li>
                        
                    </ul>
                </div>
            </div>

            <hr>

            <div class="row" id="professional-experience">
                <div class="col">
                    <h2>Professional Experience</h2>
                    <ul>
                        <li>
                            <b>(October 2019 - Present) Lecturer</b><br/>
                            Department of Computer Science and Engineering<br/>
                            Bangladesh University of Engineering and Technology (BUET)<br/>
                            <br/>
                        </li>
                        
                        <li>
                            <b>(July 2019 - Present) Graduate Research Assistant</b><br/> 
                            Department of Computer Science and Engineering<br/>
                            Bangladesh University of Engineering and Technology (BUET)<br/>
                            Supervisor: Prof. Rifat Shahriyar<br/>
                            
                        </li>

                    </ul>
                </div>
            </div>

            <hr>

            <div class="row" id="honors-awards">
                <div class="col">
                    <h2>Honors & Awards</h2>
                    <ul>
                        <li>
                            (2019 - Present) University Merit Scholarship for Graduate Students<br/>
                        </li>

                        <li>
                            (2015 - 2019) University Merit Scholarship for Undergraduate Students <br/>
                        </li>

                        <li>
                            (2015 - 2019) Faculty Dean's Merit List<br/>
                        </li>
                        
                    </ul>
                </div>
            </div>

            <hr>

            <div class="row" id="teaching-experience">
                <div class="col">
                    <h2>Teaching Experience</h2>
                    <ul>
                        <li>
                        	(January 2021) Course Instructor @ Object Oriented Programming Language Sessional (CSE 108)
                       	</li>
                        <li>
                            (January 2021) Course Instructor @ Digital Logic Design Sessional (CSE 206)
                        </li>
                        <li>
                            (January 2021) Course Instructor @ Software Engineering Sessional (CSE 308)
                        </li>
                    	<li>
                            (January 2021) Course Instructor @ Microprocessors, Microcontrollers, and Embedded Systems Sessional (CSE 316)
                        </li>                        
                        <li>
                        	(January 2020) Course Instructor @ Data Structures and Algorithms II Sessional (CSE 208)
                        </li>
                        <li>
                        	(January 2020) Course Instructor @ Computer Networks Sessional (CSE 322)
                        </li>
                        <li>
                        	(January 2020) Course Instructor @ Simulation and Modeling Sessional (CSE 412)
                        </li>
                        <li>
                        	(January 2020) Course Instructor @ Algorithm Engineering Sessional (CSE 462)
                        </li>
                        <li>
                        	(January 2020) Course Instructor @ Machine Learning Sessional (CSE 472)
                        </li>
                        <li>
                        	(July 2019) Course Instructor @ Data Structures and Algorithms I Sessional (CSE 204)
                        </li>
                        <li>
                        	(July 2019) Course Instructor @ Numerical Methods (CSE 218)
                        </li>
                        <li>
                        	(July 2019) Course Instructor @ Computer Programming Techniques Sessional (CSE 296)
                        </li>
                        <li>
                        	(July 2019) Course Instructor @ Software Development (CSE 408)
                        </li>
                    </ul>
                </div>
            </div>

            <hr>

        </div>

	    <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
             (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                                     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                                    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
            ga('create', 'UA-51640218-1', 'auto');
            ga('send', 'pageview');
        </script>
    </body>
</html>
