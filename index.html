<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Tahmid Hasan</title>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
	    <meta property="og:title" content="Tahmid Hasan" />
	    <meta property="og:image" content="https://tahmid04.github.io/img/tahmid.jpg" />
	    <meta http-equiv="X-UA-Compatible" content="IE=edge">
	    <meta name="author" content="Tahmid Hasan">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="shortcut icon" type="image/png" href="favicon.ico"/>
        <link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.0.0-beta/css/bootstrap.min.css'>
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" href="css/style.css">
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    
        <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js'></script>
        <script src='https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.0.8/popper.min.js'></script>
        <script src='https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.0.0-beta/js/bootstrap.min.js'></script>
        <script  src="js/script.js"></script>
    </head>
    <body>
    	<style>
		pre {
    		text-align: left;
    		white-space: pre-line;
  		}
		</style>

        <div class="container mt-5" >
            <nav class="navbar navbar-expand-lg navbar-dark fixed-top bg-dark">
                <div class="container">
                        <button id="myButton" class="navbar-toggler" type="button" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
                                <span class="navbar-toggler-icon"></span>
                        </button>
                        <div class="collapse navbar-collapse" id="navbarCollapse">
                        <ul class="navbar-nav">
                        <li class="nav-item active">
                            <a class="nav-link" href="#about">About <span class="sr-only">(current)</span></a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#publications">Publications</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#ongoing-projects">Ongoing Projects</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#education">Education</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#professional-experience">Professional Experience</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#honors-awards">Honors & Awards</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#teaching-experience">Teaching</a>
                        </li>
                        </ul>
                        </div>
                </div>
            </nav>
    
            <div class="row mb-3" id="about">
                <div class="col"><br/>
                    <h1>Tahmid Hasan</h1>
                </div>
            </div>

            <div class="row" style="font-size:110%;">
                <div class="col-md-4 order-md-2">
                    <img src="img/tahmid.jpg" alt="Tahmid" class="img-fluid rounded">
                </div>
                <div class="col-md-8 order-md-1" >
                    <p>
                        Hi! I am a lecturer at the <a href="https://cse.buet.ac.bd/" target="_blank">Department of Computer Science and Engineering</a> from <a href="https://www.buet.ac.bd/" target="_blank">Bangladesh University of Engineering and Technology (BUET)</a> and am affiliated with the <a href="http://csebuetnlp.github.io/" target="_blank">BUET CSE NLP Group</a>. I got my Masters degree from the same department and was fortunate to be supervised by Prof. <a href="http://rifatshahriyar.github.io/" target="_blank">Rifat Shahriyar</a>. 

                    </p>
                    <p>
                        Previously, I got my Bachelors degree also from here and worked on Bioinformatics and Networking supervised by Prof. <a href="https://msrahman.buet.ac.bd/" target="_blank">M. Sohel Rahman</a>.
                    </p>
                    <p>
                        My current research interests are in low-resource, multilingual, and cross-lingual natural language processing. I am particularly interested in efficient utilization of compute and data in scenarios where one or both are limited. My long term goal is to create practical and general-purpose NLP systems that can learn to communicate with speakers of any language with minimal supervision.
                    </p>
                    <p>
                        Over the last three years, I have worked on multiple projects on machine translation, text summarization, natural language understanding and generation, and NLP for programming languages.
                    </p>
                </div>
            </div>

            <div class="row" style="font-size:110%;">
                <div class="col">
                    <p>
                        Email: tahmidhasan [<a href="https://en.wikipedia.org/wiki/At_sign" target="_blank">at</a>] cse.buet.ac.bd
                    </p>
                    <p>
                        Links:
                        [<a href="pdf/CV_Tahmid_Hasan.pdf" target="_blank">Curriculum Vitae</a>] [<a href="https://scholar.google.com/citations?user=UfXXUpQAAAAJ&hl=en" target="_blank">Google Scholar</a>] [<a href="https://twitter.com/tahmidhasan04" target="_blank">Twitter</a>] [<a href="https://github.com/Tahmid04" target="_blank">GitHub</a>]
                    </p>
                </div>
            </div>
            
            <hr>

            <div class="row" id="publications">
                <div class="col">
                    <h2>Publications</h2>
                    <ol>
                        <li>

                            <a href="https://aclanthology.org/2021.findings-acl.413.pdf" target="_blank">
                                <b>XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages</b>
                            </a>
                            <br/>
                            <b>Tahmid Hasan</b>,  Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, Rifat Shahriyar
                            <br/>
                            In <a href="https://aclanthology.org/volumes/2021.findings-acl/" target="_blank">
                                <b>
                                    Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 </b></a>
                            <br/>               

                            [<a href="#" onclick="$('#acl2021_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#acl2021_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://github.com/csebuetnlp/xl-sum" target="_blank">code</a>]
                            <div id="acl2021_abstract" class="abstract" style="display:none;">
                                <p>
                                    Contemporary works on abstractive text summarization have focused primarily on high-resource languages like English, mostly due to the limited availability of datasets for low/mid-resource ones. In this work, we present XL-Sum, a comprehensive and diverse dataset comprising 1 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 44 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation. We fine-tune mT5, a state-of-the-art pretrained multilingual model, with XL-Sum and experiment on multilingual and low-resource summarization tasks. XL-Sum induces competitive results compared to the ones obtained using similar monolingual datasets: we show higher than 11 ROUGE-2 scores on 10 languages we benchmark on, with some of them exceeding 15, as obtained by multilingual training. Additionally, training on low-resource languages individually also provides competitive performance. To the best of our knowledge, XL-Sum is the largest abstractive summarization dataset in terms of the number of samples collected from a single source and the number of languages covered. We are releasing our dataset and models to encourage future research on multilingual abstractive summarization. The resources can be found at <a href="https://github.com/csebuetnlp/xl-sum" target="_blank">https://github.com/csebuetnlp/xl-sum</a>.
                                </p>
                            </div>
                            <div id="acl2021_bib" class="bib" style="display:none;">
                                <pre>
                                    @inproceedings{hasan-etal-2021-xl,
                                        title = "{XL}-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages",
                                        author = "Hasan, Tahmid  and
                                            Bhattacharjee, Abhik  and
                                            Islam, Md. Saiful  and
                                            Mubasshir, Kazi  and
                                            Li, Yuan-Fang  and
                                            Kang, Yong-Bin  and
                                            Rahman, M. Sohel  and
                                            Shahriyar, Rifat",
                                        booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                                        month = aug,
                                        year = "2021",
                                        address = "Online",
                                        publisher = "Association for Computational Linguistics",
                                        url = "https://aclanthology.org/2021.findings-acl.413",
                                        doi = "10.18653/v1/2021.findings-acl.413",
                                        pages = "4693--4703",
                                    }
                                </pre>
                            </div>
                        </li>
                        <br/>

                        <li>

                            <a href="https://arxiv.org/pdf/2101.00204.pdf" target="_blank">
                                <b>BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource Language Understanding Evaluation in Bangla</b>
                            </a>
                            <br/>
                            Abhik Bhattacharjee*, <b>Tahmid Hasan</b>*, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, M. Sohel Rahman, Anindya Iqbal, Rifat Shahriyar
                            <br/>
                            *: Equal contribution
                            <br/>
                            <a href="https://arxiv.org/abs/2101.00204" target="_blank">
                                <b>
                                    Findings of the North American Chapter of the Association for Computational Linguistics: NAACL 2022 </b></a>
                            <br/>

                            [<a href="#" onclick="$('#banglabert_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#banglabert_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://github.com/csebuetnlp/banglabert" target="_blank">code</a>]
                            <div id="banglabert_abstract" class="abstract" style="display:none;">
                                <p>
                                    In this work, we introduce BanglaBERT, a BERT-based Natural Language Understanding (NLU) model pretrained in Bangla, a widely spoken yet low-resource language in the NLP literature. To pretrain BanglaBERT, we collect 27.5 GB of Bangla pretraining data (dubbed'Bangla2B+') by crawling 110 popular Bangla sites. We introduce two downstream task datasets on natural language inference and question answering and benchmark on four diverse NLU tasks covering text classification, sequence labeling, and span prediction. In the process, we bring them under the first-ever Bangla Language Understanding Benchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming multilingual and monolingual models. We are making the models, datasets, and a leaderboard publicly available at <a href="https://github.com/csebuetnlp/banglabert" target="_blank">https://github.com/csebuetnlp/banglabert</a> to advance Bangla NLP .
                                </p>
                            </div>
                            <div id="banglabert_bib" class="bib" style="display:none;">
                                <pre>
                                    @inproceedings{bhattacharjee-etal-2022-banglabert,
                                        title     = {BanglaBERT: Lagnuage Model Pretraining and Benchmarks for Low-Resource Language Understanding Evaluation in Bangla},
                                        author = "Bhattacharjee, Abhik  and
                                          Hasan, Tahmid  and
                                          Uddin, Wasi Ahmad  and
                                          Mubasshir, Kazi  and
                                          Islam, Md. Saiful  and
                                          Iqbal, Anindya  and
                                          Rahman, M. Sohel  and
                                          Shahriyar, Rifat",
                                          booktitle = "Findings of the North American Chapter of the Association for Computational Linguistics: NAACL 2022",
                                          month = july,
                                        year      = {2022},
                                        url       = {https://arxiv.org/abs/2101.00204}
                                    }
                                </pre>
                            </div>
                        </li>
                        <br/>

                        <li>

                            <a href="https://aclanthology.org/2021.findings-acl.18.pdf" target="_blank">
                                <b>CoDesc: A Large Code–Description Parallel Dataset</b>
                            </a>
                            <br/>
                            Masum Hasan, Tanveer Muttaqueen, Abdullah Al Ishtiaq, Kazi Sajeed Mehrab, Md. Mahim Anjum Haque, <b>Tahmid Hasan</b>, Wasi Ahmad, Anindya Iqbal, Rifat Shahriyar
                            <br/>
                            In <a href="https://aclanthology.org/volumes/2021.findings-acl/" target="_blank">
                                <b>
                                    Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 </b></a>
                            <br/>

                            [<a href="#" onclick="$('#aclcodesc2021_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#aclcodesc2021_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://github.com/csebuetnlp/CoDesc" target="_blank">code</a>]
                            <div id="aclcodesc2021_abstract" class="abstract" style="display:none;">
                                <p>
                                    Translation between natural language and source code can help software development by enabling developers to comprehend, ideate, search, and write computer programs in natural language. Despite growing interest from the industry and the research community, this task is often difficult due to the lack of large standard datasets suitable for training deep neural models, standard noise removal methods, and evaluation benchmarks. This leaves researchers to collect new small-scale datasets, resulting in inconsistencies across published works. In this study, we present CoDesc -- a large parallel dataset composed of 4.2 million Java methods and natural language descriptions. With extensive analysis, we identify and remove prevailing noise patterns from the dataset. We demonstrate the proficiency of CoDesc in two complementary tasks for code-description pairs: code summarization and code search. We show that the dataset helps improve code search by up to 22\% and achieves the new state-of-the-art in code summarization. Furthermore, we show CoDesc's effectiveness in pre-training--fine-tuning setup, opening possibilities in building pretrained language models for Java. To facilitate future research, we release the dataset, a data processing tool, and a benchmark at <a href="https://github.com/csebuetnlp/CoDesc" target="_blank">https://github.com/csebuetnlp/CoDesc</a>.
                                </p>
                            </div>
                            <div id="aclcodesc2021_bib" class="bib" style="display:none;">
                                <pre>
                                    @inproceedings{hasan-etal-2021-codesc,
                                        title = "{C}o{D}esc: A Large Code{--}Description Parallel Dataset",
                                        author = "Hasan, Masum  and
                                          Muttaqueen, Tanveer  and
                                          Ishtiaq, Abdullah Al  and
                                          Mehrab, Kazi Sajeed  and
                                          Haque, Md. Mahim Anjum  and
                                          Hasan, Tahmid  and
                                          Ahmad, Wasi  and
                                          Iqbal, Anindya  and
                                          Shahriyar, Rifat",
                                        booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                                        month = aug,
                                        year = "2021",
                                        address = "Online",
                                        publisher = "Association for Computational Linguistics",
                                        url = "https://aclanthology.org/2021.findings-acl.18",
                                        doi = "10.18653/v1/2021.findings-acl.18",
                                        pages = "210--218",
                                    }
                                </pre>
                            </div>
                        </li>
                        <br/>

                        <li>

                            <a href="https://aclanthology.org/2020.emnlp-main.207.pdf" target="_blank">
                                <b>Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New Datasets for Bengali-English Machine Translation</b>
                            </a>
                            <br/>
                            <b>Tahmid Hasan</b>,  Abhik Bhattacharjee,  Kazi Samin, Masum Hasan, Madhusudan Basak, M. Sohel Rahman, Rifat Shahriyar     
                            <br/>
                            In <a href="https://aclanthology.org/volumes/2020.emnlp-main/" target="_blank">
                                <b>
                                    Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) </b></a>
                            <br/>

                            [<a href="#" onclick="$('#emnlp2020_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#emnlp2020_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://github.com/csebuetnlp/banglanmt" target="_blank">code</a>]
                            <div id="emnlp2020_abstract" class="abstract" style="display:none;">
                                <p>
                                    Despite being the seventh most widely spoken language in the world, Bengali has received much less attention in machine translation literature due to being low in resources. Most publicly available parallel corpora for Bengali are not large enough; and have rather poor quality, mostly because of incorrect sentence alignments resulting from erroneous sentence segmentation, and also because of a high volume of noise present in them. In this work, we build a customized sentence segmenter for Bengali and propose two novel methods for parallel corpus creation on low-resource setups: aligner ensembling and batch filtering. With the segmenter and the two methods combined, we compile a high-quality Bengali-English parallel corpus comprising of 2.75 million sentence pairs, more than 2 million of which were not available before. Training on neural models, we achieve an improvement of more than 9 BLEU score over previous approaches to Bengali-English machine translation. We also evaluate on a new test set of 1000 pairs made with extensive quality control. We release the segmenter, parallel corpus, and the evaluation set, thus elevating Bengali from its low-resource status. To the best of our knowledge, this is the first ever large scale study on Bengali-English machine translation. We believe our study will pave the way for future research on Bengali-English machine translation as well as other low-resource languages. Our data and code are available at <a href="https://github.com/csebuetnlp/banglanmt" target="_blank">https://github.com/csebuetnlp/banglanmt</a>.
                                </p>
                            </div>
                            <div id="emnlp2020_bib" class="bib" style="display:none;">
                                <pre>
                                    @inproceedings{hasan-etal-2020-low,
                                        title = "Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New Datasets for {B}engali-{E}nglish Machine Translation",
                                        author = "Hasan, Tahmid  and
                                          Bhattacharjee, Abhik  and
                                          Samin, Kazi  and
                                          Hasan, Masum  and
                                          Basak, Madhusudan  and
                                          Rahman, M. Sohel  and
                                          Shahriyar, Rifat",
                                        booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                                        month = nov,
                                        year = "2020",
                                        address = "Online",
                                        publisher = "Association for Computational Linguistics",
                                        url = "https://aclanthology.org/2020.emnlp-main.207",
                                        doi = "10.18653/v1/2020.emnlp-main.207",
                                        pages = "2612--2623",                                        
                                    }
                                </pre>
                            </div>
                        </li>
                        <br/>


                        <li>

                            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8214213" target="_blank">
                                <b>Using Adaptive Heartbeat Rate on Long-Lived TCP Connections</b>
                            </a>
                            <br/>
                            M. Saifur Rahman, Md. Yusuf Sarwar Uddin, <b>Tahmid Hasan</b>, M. Sohel Rahman, M. Kaykobad
                            <br/>
                            In <a href="https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=8291715" target="_blank">
                                <b>
                                    IEEE/ACM Transactions on Networking (Volume: 26, Issue: 1, Feb. 2018) </b></a>
                            <br/>

                            [<a href="#" onclick="$('#ton2018_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#ton2018_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://github.com/srautonu/AdaptiveHeartBeat" target="_blank">code</a>]
                            <div id="ton2018_abstract" class="abstract" style="display:none;">
                                <p>
                                    In this paper, we propose techniques for dynamically adjusting heartbeat or keep-alive interval of long-lived TCP connections, particularly the ones that are used in push notification service in mobile platforms. When a device connects to a server using TCP, often times the connection is established through some sort of middle-box, such as NAT, proxy, firewall, and so on. When such a connection is idle for a long time, it may get torn down due to binding timeout of the middle-box. To keep the connection alive, the client device needs to send keep-alive packets through the connection when it is otherwise idle. To reduce resource consumption, the keep-alive packet should preferably be sent at the farthest possible time within the binding timeout. Due to varied settings of different network equipments, the binding timeout will not be identical in different networks. Hence, the heartbeat rate used in different networks should be changed dynamically. We propose a set of iterative probing techniques, namely binary, exponential, and composite search, that detect the middle-box binding timeout with varying degree of accuracy; and in the process, keeps improving the keep-alive interval used by the client device. We also analytically derive performance bounds of these techniques. To the best of our knowledge, ours is the first work that systematically studies several techniques to dynamically improve keep-alive interval. To this end, we run experiments in simulation as well as make a real implementation on android to demonstrate the proof-of-concept of the proposed schemes.
                                </p>
                            </div>
                            <div id="ton2018_bib" class="bib" style="display:none;">
                                <pre>
                                    @article{rahman2017using,
                                        title={Using adaptive heartbeat rate on long-lived TCP connections},
                                        author={Rahman, M Saifur and Uddin, Md Yusuf Sarwar and Hasan, Tahmid and Rahman, M Sohel and Kaykobad, M},
                                        journal={IEEE/ACM Transactions on Networking},
                                        volume={26},
                                        number={1},
                                        pages={203--216},
                                        year={2017},
                                        publisher={IEEE}
                                      }
                                </pre>
                            </div>
                        </li>
                        <br/>

                    </ol>
       
                    <h5>Pre-prints:</h5>
                    <ol>
                        <li>

                            <a href="pdf/BanglaNLG-preprint.pdf" target="_blank">
                                <b>BanglaNLG: Benchmarks and Resources for Evaluating Low-Resource Natural Language Generation in Bangla</b>
                            </a>
                            <br/>
                            Abhik Bhattacharjee, <b>Tahmid Hasan</b>, Wasi Uddin Ahmad, Rifat Shahriyar
                            <br/>
                            <a href="https://arxiv.org/pdf/2205.11081" target="_blank">
                                <b>
                                    ArXiv Pre-print, 2022 </b></a>
                            <br/>

                            [<a href="#" onclick="$('#banglanlg_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#banglanlg_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://github.com/csebuetnlp/BanglaNLG" target="_blank">code</a>]
                            <div id="banglanlg_abstract" class="abstract" style="display:none;">
                                <p>
                                    This work presents BanglaNLG, a comprehensive benchmark for evaluating natural language generation (NLG) models in Bangla, a widely spoken yet low-resource language in the web domain. We aggregate three challenging conditional text generation tasks under the BanglaNLG benchmark. Then, using a clean corpus of 27.5 GB of Bangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer model for Bangla. BanglaT5 achieves state-of-the-art performance in all of these tasks, outperforming mT5 (base) by up to 5.4%. We are making the BanglaT5 language model and a leaderboard publicly available in the hope of advancing future research and evaluation on Bangla NLG. The resources can be found at <a href="https://github.com/csebuetnlp/BanglaNLG" target="_blank">https://github.com/csebuetnlp/BanglaNLG</a>.
                                </p>
                            </div>
                            <div id="banglanlg_bib" class="bib" style="display:none;">
                                <pre>
                                    @article{bhattacharjee2022banglanlg,
                                        author    = {Abhik Bhattacharjee and Tahmid Hasan and Wasi Uddin Ahmad and Rifat Shahriyar},
                                        title     = {BanglaNLG: Benchmarks and Resources for Evaluating Low-Resource Natural Language Generation in Bangla},
                                        journal   = {CoRR},
                                        volume    = {abs/2205.11081},
                                        year      = {2022},
                                        url       = {https://arxiv.org/abs/2205.11081},
                                        eprinttype = {arXiv},
                                        eprint    = {2205.11081}
                                      }
                                </pre>
                            </div>
                        </li>
                        <br/>
                        
                        <li>

                            <a href="pdf/CrossSum-preprint.pdf" target="_blank">
                                <b>CrossSum: Beyond English-Centric Cross-Lingual Abstractive Text Summarization for 1500+ Language Pairs</b>
                            </a>
                            <br/>
                            <b>Tahmid Hasan</b>, Abhik Bhattacharjee, Wasi Uddin Ahmad, Yuan-Fang Li, Yong-Bin Kang, Rifat Shahriyar
                            <br/>
                            <a href="https://arxiv.org/abs/2112.08804" target="_blank">
                                <b>
                                    ArXiv Pre-print, 2021 </b></a>
                            <br/>

                            [<a href="#" onclick="$('#crosssum_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#crosssum_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://github.com/csebuetnlp/CrossSum" target="_blank">code</a>]
                            <div id="crosssum_abstract" class="abstract" style="display:none;">
                                <p>
                                    We present CrossSum, a large-scale cross-lingual abstractive summarization dataset comprising 1.7 million article-summary samples in 1500+ language pairs. We create CrossSum by aligning identical articles written in different languages via cross-lingual retrieval from a multilingual summarization dataset. We propose a multi-stage data sampling algorithm to effectively train a cross-lingual summarization model capable of summarizing an article in any target language. We also propose LaSE, a new metric for automatically evaluating model-generated summaries and showing a strong correlation with ROUGE. Performance on ROUGE and LaSE indicate that pretrained models fine-tuned on CrossSum consistently outperform baseline models, even when the source and target language pairs are linguistically distant. To the best of our knowledge, CrossSum is the largest cross-lingual summarization dataset and the first-ever that does not rely solely on English as the pivot language. We are releasing the dataset, alignment and training scripts, and the models to spur future research on cross-lingual abstractive summarization. The resources can be found <a href="https://github.com/csebuetnlp/CrossSum" target="_blank">https://github.com/csebuetnlp/CrossSum</a>.
                                </p>
                            </div>
                            <div id="crosssum_bib" class="bib" style="display:none;">
                                <pre>
                                    @article{hasan2021crosssum,
                                        title={CrossSum: Beyond English-Centric Cross-Lingual Abstractive Text Summarization for 1500+ Language Pairs},
                                        author={Hasan, Tahmid and Bhattacharjee, Abhik and Li, Yuan-Fang and Kang, Yong-Bin and and Shahriyar, Rifat},
                                        journal={arXiv preprint arXiv:2112.08804},
                                        year={2021}
                                    }
                                </pre>
                            </div>
                        </li>
                        <br/>
                        
                        <li>

                            <a href="https://arxiv.org/pdf/2104.08017.pdf" target="_blank">
                                <b>BERT2Code: Can Pretrained Language Models be Leveraged for Code Search?</b>
                            </a>
                            <br/>
                            Abdullah Al Ishtiaq, Masum Hasan, Md. Mahim Anjum Haque, Kazi Sajeed Mehrab, Tanveer Muttaqueen, <b>Tahmid Hasan</b>, Anindya Iqbal, Rifat Shahriyar
                            <br/>
                            <a href="https://arxiv.org/abs/2104.08017" target="_blank">
                                <b>
                                    ArXiv Pre-print, 2021 </b></a>
                            <br/>               

                            [<a href="#" onclick="$('#bert2code_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#bert2code_abstract').toggle();return false;">abstract</a>]
                            <div id="bert2code_abstract" class="abstract" style="display:none;">
                                <p>
                                    Millions of repetitive code snippets are submitted to code repositories every day. To search from these large codebases using simple natural language queries would allow programmers to ideate, prototype, and develop easier and faster. Although the existing methods have shown good performance in searching codes when the natural language description contains keywords from the code, they are still far behind in searching codes based on the semantic meaning of the natural language query and semantic structure of the code. In recent years, both natural language and programming language research communities have created techniques to embed them in vector spaces. In this work, we leverage the efficacy of these embedding models using a simple, lightweight 2-layer neural network in the task of semantic code search. We show that our model learns the inherent relationship between the embedding spaces and further probes into the scope of improvement by empirically analyzing the embedding methods. In this analysis, we show that the quality of the code embedding model is the bottleneck for our model's performance, and discuss future directions of study in this area.
                                </p>
                            </div>
                            <div id="bert2code_bib" class="bib" style="display:none;">
                                <pre>
                                    @article{ishtiaq2021bert2code,
                                        title={BERT2Code: Can Pretrained Language Models be Leveraged for Code Search?},
                                        author={Ishtiaq, Abdullah Al and Hasan, Masum and Haque, Md and Anjum, Mahim and Mehrab, Kazi Sajeed and Muttaqueen, Tanveer and Hasan, Tahmid and Iqbal, Anindya and Shahriyar, Rifat},
                                        journal={arXiv preprint arXiv:2104.08017},
                                        year={2021}
                                    }
                                </pre>
                            </div>
                        </li>
                        <br/>

                    </ol>
                </div>
            </div>

            <hr>

            <div class="row" id="ongoing-projects">
                <div class="col">
                    <h2>Ongoing Projects</h2>
                    <ol>
                        <li>
                            <b>Adapting XL-Sum for Cross-Lingual Summarization</b>
                            [<a href="#" onclick="$('#xlcross_statement').toggle();return false;">statement</a>]                            
                            <div id="xlcross_statement" class="statement" style="display:none;">
                                <p>
                                    The target language of a multilingual model for cross-lingual summarization is limited to only the language it is fine-tuned on, and we have observed that fine-tuning with multiple languages without cross-lingual supervision can not help control the language of the generated summaries. In this work, we are aiming to generate summaries in any target language for a given article by fine-tuning multilingual models with explicit (albeit limited) cross-lingual signals. By aligning identical articles across languages via cross-lingual retrieval on the <a href="https://github.com/csebuetnlp/xl-sum" target="_blank">XL-Sum dataset</a>, coupled with a multi-stage sampling technique, we are aiming to perform large-scale cross-lingual summarization for 45 languages.
                                </p>
                            </div>
                            <br/>
                            Supervisors: Dr. <a href="https://research.monash.edu/en/persons/yuan-fang-li" target="_blank">Yuan-Fang Li</a> and Prof. Rifat Shahriyar<br/>
                            <br/>
                        </li>

                        <li>
                            <b>Paraphrase Generation via Knowledge Distillation from Machine Translation Models</b>
                            [<a href="#" onclick="$('#kd_statement').toggle();return false;">statement</a>]                            
                            <div id="kd_statement" class="statement" style="display:none;">
                                <p>
                                    Synthetic paraphrase datasets are typically generated with round-trip machine translation. Since these back-translation-based data generation approaches have been shown to generate appropriate paraphrases, in this work, we are trying to directly distill the knowledge of translation models into a paraphrase generation model. We are aiming to use two teachers, namely a forward translation model and a backward translation model, to distill two types of knowledge into the paraphrase model: the cross-attention distribution and the output distribution. In constrast to traditional knowledge distillation, here we have two teacher models instead of one and the task of the student model is different from the teacher models.
                                </p>
                            </div>
                            <br/>
                            Supervisors: Dr. <a href="https://wasiahmad.github.io/" target="_blank">Wasi Uddin  Ahmad</a> and Prof. Rifat Shahriyar<br/>
                            <br/>
                        </li>
                        
                    </ol>
                </div>
            </div>

            <hr>

            <div class="row" id="education">
                <div class="col">
                    <h2>Education</h2>
                    <ul>
                        <li>
                            <b>(June 2019 - May 2022) Master of Science in Computer Science and Engineering</b><br/>
                            Department of Computer Science and Engineering<br/>
                            Bangladesh University of Engineering and Technology (BUET)<br/>
                            Supervisor: Prof. Rifat Shahriyar<br/>
                            CGPA: 3.92/4.00<br/>
                            <br/>
                        </li>

                        <li>
                            <b>(February 2015 - April 2019) Bachelor of Science in Computer Science and Engineering</b><br/>
                            Department of Computer Science and Engineering<br/>
                            Bangladesh University of Engineering and Technology (BUET)<br/>
                            CGPA: 3.98/4.00<br/>
                            Thesis Supervisor: Prof. M. Sohel Rahman
                            <br/>
                        </li>
                        
                    </ul>
                </div>
            </div>

            <hr>

            <div class="row" id="professional-experience">
                <div class="col">
                    <h2>Professional Experience</h2>
                    <ul>
                        <li>
                            <b>(October 2019 - Present) Lecturer</b><br/>
                            Department of Computer Science and Engineering<br/>
                            Bangladesh University of Engineering and Technology (BUET)<br/>
                            <br/>
                        </li>

                        <li>
                            <b>(June 2022 - Present) Research Affiliate</b><br/>
                            BUET CSE NLP Group<br/> 
                            Supervisor: Prof. Rifat Shahriyar<br/><br/>
                            
                        </li>
                        
                        <li>
                            <b>(June 2019 - May 2022) Graduate Research Assistant</b><br/> 
                            Department of Computer Science and Engineering<br/>
                            Bangladesh University of Engineering and Technology (BUET)<br/>
                            Supervisor: Prof. Rifat Shahriyar<br/>
                            
                        </li>

                    </ul>
                </div>
            </div>

            <hr>

            <div class="row" id="honors-awards">
                <div class="col">
                    <h2>Honors & Awards</h2>
                    <ul>
                        <li>
                            (2019 - 2022) University Merit Scholarships in each semester for excellent postgraduate results<br/>
                        </li>

                        <li>
                            (2015 - 2019) Dean's Award in each academic year for excellent undergraduate results<br/>
                        </li>

                        <li>
                            (2015 - 2019) University Merit Scholarships in each semester for excellent undergraduate results<br/>
                        </li>
                        
                    </ul>
                </div>
            </div>

            <hr>

            <div class="row" id="teaching-experience">
                <div class="col">
                    <h2>Teaching Experience</h2>
                    <ul>
                        <li>
                        	(July 2022) Course Instructor @ Mathematical Analysis for Computer Science (CSE 301)
                       	</li>
                        <li>
                        	(July 2022) Course Instructor @ Mathematical Analysis for Computer Science (CSE 218)
                       	</li>
                        <li>
                        	(July 2022) Course Instructor @ Mathematical Analysis for Computer Science (CSE 314)
                       	</li>
                        <li>
                        	(January 2022) Course Instructor @ Computer Architecture (CSE 305)
                       	</li>
                        <li>
                        	(January 2022) Course Instructor @ Computer Architecture Sessional (CSE 306)
                       	</li>
                        <li>
                        	(January 2022) Course Instructor @ Software Development (CSE 408)
                       	</li>
                        <li>
                        	(January 2022) Course Instructor @ Object Oriented Programming Language Sessional (CSE 108)
                       	</li>
                        <li>
                        	(July 2021) Course Instructor @ Machine Learning (CSE 471)
                       	</li>
                        <li>
                        	(July 2021) Course Instructor @ Data Structures and Algorithms I Sessional (CSE 204)
                       	</li>
                        <li>
                        	(July 2021) Course Instructor @ Database Sessional (CSE 216)
                       	</li>
                        <li>
                        	(July 2021) Course Instructor @ Numerical Methods Sessional (CSE 218)
                       	</li>
                        <li>
                        	(July 2021) Course Instructor @ Machine Learning Sessional (CSE 472)
                       	</li>
                        <li>
                        	(January 2021) Course Instructor @ Object Oriented Programming Language Sessional (CSE 108)
                       	</li>
                        <li>
                            (January 2021) Course Instructor @ Digital Logic Design Sessional (CSE 206)
                        </li>
                        <li>
                            (January 2021) Course Instructor @ Software Engineering Sessional (CSE 308)
                        </li>
                    	<li>
                            (January 2021) Course Instructor @ Microprocessors, Microcontrollers, and Embedded Systems Sessional (CSE 316)
                        </li>                        
                        <li>
                        	(January 2020) Course Instructor @ Data Structures and Algorithms II Sessional (CSE 208)
                        </li>
                        <li>
                        	(January 2020) Course Instructor @ Computer Networks Sessional (CSE 322)
                        </li>
                        <li>
                        	(January 2020) Course Instructor @ Simulation and Modeling Sessional (CSE 412)
                        </li>
                        <li>
                        	(January 2020) Course Instructor @ Algorithm Engineering Sessional (CSE 462)
                        </li>
                        <li>
                        	(January 2020) Course Instructor @ Machine Learning Sessional (CSE 472)
                        </li>
                        <li>
                        	(July 2019) Course Instructor @ Data Structures and Algorithms I Sessional (CSE 204)
                        </li>
                        <li>
                        	(July 2019) Course Instructor @ Numerical Methods (CSE 218)
                        </li>
                        <li>
                        	(July 2019) Course Instructor @ Computer Programming Techniques Sessional (CSE 296)
                        </li>
                        <li>
                        	(July 2019) Course Instructor @ Software Development (CSE 408)
                        </li>
                    </ul>
                </div>
            </div>

            <hr>

        </div>

	    <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
             (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                                     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                                    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
            ga('create', 'UA-51640218-1', 'auto');
            ga('send', 'pageview');
        </script>
    </body>
</html>
